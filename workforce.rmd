---
title: "Chose Your Own Project - Machine Learning Submission"
subtitle: "HarvardX Data Science Capstone - PH125.9x"
author: "Simon Gibson"
date: "`r Sys.Date()`"
output: pdf_document
number_sections: true
toc: true
---
  
## Introduction

For the 9th Course in the HarvardX Data Science course we have been asked to create two recommendation systems. The first was a Movie Recommendation System using the MovieLens dataset. The second is a "Choose your Own Project." For this a we are targetting a Workforce Recommendation System - mixing weather forecasts with Police 911 call information to see if it is possible to predict Police staffing requirements based on weather based trends.

We are using the Seattle Police Department 911 Incident Response data set found here : https://www.kaggle.com/datasets/sohier/seattle-police-department-911-incident-response

For Weather data we will use National Oceanic and Atmospheric Administration (NOAA) data. Michael Minns' tutorial is inciteful for weather analysis. It can be found here: https://michaelminn.net/tutorials/r-weather/index.html This weather data does not appear to be available via an api call or similar and is quite a manual download process. Due to download constraints we will be using a locally sourced dataset covering the years 2001 to 2002.

In order to test the results of the recommendation system we are using the root-mean-square error (RMSE) to measure the difference between the values predicted by the model and the observed values.

## Method

The first step is to clear any set variables so we do not introduce anything unexpected into the data we are working with.

```{r clear, echo=FALSE}
#clear R variables
#rm(list = ls ())
```

Then we install the packages required to manipulate the data.

```{r packageInstall, echo=TRUE, results="hide", message=FALSE}
####################################################
# This code is divided into the following sections #
# 1. Install required packages                     #
# 2. edx code for creating data sets               #
# 3. Data set exploration                          #
####################################################

##########################################################
# 1. Install required packages and download data            #
##########################################################

# Note: this process takes a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "https://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "https://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "https://cran.us.r-project.org") 
if(!require(kableExtra)) install.packages("kableExtra", repos = "https://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "https://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "https://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(xts)) install.packages("xts", repos = "http://cran.us.r-project.org")
if(!require(tsbox)) install.packages("tsbox", repos = "http://cran.us.r-project.org")
if(!require(forecast)) install.packages("forecast", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(measurements)) install.packages("measurements", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(ggmap)) install.packages("ggmap", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(dplyr)
library(kableExtra)
library(lubridate)
library(scales)
library(stringr)
library(readr)
library(xts)
library(tsbox)
library(forecast)
library(data.table)
library(measurements)
library(kableExtra)
library(ggmap)

```

Following that, the data is downloaded and then divided into 2 sets. The first set is used to train the algorithm and the second set is used to validate the algorithm. By dividing the data the problem of over-training and thus producing skewed results can be avoided.

The creation of the 2 sets involves the following steps. Initially required packages are installed if not installed and then loaded. Next the data is downloaded if the zip files are not found. Column names are set and the data is converted into forms more easily processed. Then the data is joined. Finally the joined data is split into 2 sets - the edx set used to train the algorithm and the final_holdout_test set that will be used to validate the algorithm and calculate the final RMSE score.

```{r dataDownload, echo=TRUE, results="hide", message=FALSE, warning=FALSE}

#Seattle Police Department 911 Incident Response
#https://www.kaggle.com/datasets/sohier/seattle-police-department-911-incident-response/download?datasetVersionNumber=1

#National Oceanic and Atmospheric Administration (NOAA) data
#https://www.ncei.noaa.gov/orders/cdo/3533326.csv

options(timeout = 120)

dl <- "archive.zip"
if(!file.exists(dl))
  download.file("https://www.kaggle.com/datasets/sohier/seattle-police-department-911-incident-response/download?datasetVersionNumber=1", dl)

dl <- "3533326.csv"
if(!file.exists(dl))
  download.file("https://www.ncei.noaa.gov/orders/cdo/3533326.csv", dl)

```

``` {r loadData, echo=TRUE, results="hide", message=FALSE, warning=TRUE}

#Load Seattle 0911 Call data; Remove spaces from Column Labels
Seattle_911 <- read.csv("Seattle_Police_Department_911_Incident_Response.csv", check.names=TRUE)
#Remove spaces from variable names
Seattle_911
#Load weather data
Weather <- read.csv("3533326.csv", as.is=T)


```

##Data Investigation

``` {r dataInvWeather}
head(Weather)

names(Weather)

min(range(Weather$DATE))

max(range(Weather$DATE))
```
Our data range starts from `r min(range(Weather$DATE))` and ends `r max(range(Weather$DATE))`. 
``` {r Seattle_Weather}
#Seattle_Weather <- xts(Weather["Weather$STATION" == 'USC00450872',c("TMAX","TMIN","PRCP")], order.by=as.Date(Weather$DATE))
Seattle_Weather <- xts(Weather[,c("NAME","STATION","DATE","TMAX","TMIN","PRCP")], order.by=as.Date(Weather$DATE))

Seattle_Weather <- as.data.frame(Seattle_Weather)
#Seattle_Weather = window(Seattle_Weather, start=as.Date("2000-01-01"), end=as.Date("2002-12-31"))

class(Seattle_Weather)
Seattle_Weather$DATE <- as.Date(Seattle_Weather$DATE)
Seattle_Weather$PRCP <- as.numeric(Seattle_Weather$PRCP)

#Convert Precipitation from Imperial to Metric
Seattle_Weather$PRCP <- conv_unit(Seattle_Weather$PRCP, "inch", "mm")


Seattle_Weather$TMAX <- as.numeric(Seattle_Weather$TMAX)
Seattle_Weather$TMAX <- conv_unit(Seattle_Weather$TMAX, "F", "C")

Seattle_Weather$TMIN <- as.numeric(Seattle_Weather$TMIN)
Seattle_Weather$TMIN <- conv_unit(Seattle_Weather$TMIN, "F", "C")

#Extract Unique Station Names and Identifiers
Seattle_Stations <- unique(Seattle_Weather[, c('NAME', 'STATION')])

# Remove the index column - otherwise it gets printed even though we asked for only Station and Name
rownames(Seattle_Stations) <- NULL

ggplot(Seattle_Weather, aes(x=DATE,y=PRCP)) +
  geom_line() +
  xlab("Date") +
  ylab("Daily Rainfall (Millimeters)") 

options(digits=2)

# https://console.cloud.google.com/google/maps-apis/credentials key. Set in .rProfile 
register_google(key = APIKEY)

# Create the dataframe
locations <- data.frame(
  Location = c("BREMERTON, WA US", "EVERETT, WA US", "MONROE, WA US", 
               "TOLT SOUTH FORK RESERVOIR, WA US", "RENTON MUNICIPAL AIRPORT, WA US", 
               "KENT, WA US", "TACOMA NUMBER 1, WA US", "LANDSBURG, WA US", 
               "CEDAR LAKE, WA US", "SNOQUALMIE FALLS, WA US", "WAUNA 3 W, WA US", 
               "PALMER 3 ESE, WA US"),
  ID = c("USC00450872", "USC00452675", "USC00455525", "USC00458508", 
         "USW00094248", "USC00454169", "USC00458278", "USC00454486", 
         "USC00451233", "USC00457773", "USC00459021", "USC00456295")
)

# Geocode the locations to add latitude and longitude
geocoded_data <- locations %>%
  mutate_geocode(Location, output = "latlona", source = "google")

# View the result
print(geocoded_data)
dl <- "geocoded_stations.csv"
if(file.exists(dl)){
  geocoded_data <- read.csv("geocoded_stations.csv", as.is=T)
} else{
  options(digits=10)
  #call google maps api to get lat and longitude of stations; returned lat and long is accurate to 2 decimal places - within 1.1 kms.
  geocoded_data <- Seattle_Stations %>%
    mutate_geocode(NAME, output = "latlona", source = "google")
  #write locations to file - latitude and longitude shouldn't change. 
  write_csv(geocoded_data, file = "geocoded_stations.csv")
}


```
We have data from `r n_distinct(Weather$STATION)` stations: 

`r kable(Seattle_Stations, format = "markdown")` 

Of `r length(Seattle_Weather$PRCP)` rainfall measurements, `r length(which(Seattle_Weather$PRCP!=0))` recorded rainfall, and `r length(which(Seattle_Weather$PRCP==0))` recorded no rainfall. The maximum rainfall during this period was `r max(Seattle_Weather$PRCP, na.rm =T)`mm which fell on `r Seattle_Weather[which.max(Seattle_Weather$PRCP), "DATE"]`. Heavy rainfall is defined by NIWA as rainfall of over 100mm in 24 hours^[https://niwa.co.nz/natural-hazards/extreme-weather-heavy-rainfall] and this occurred  `r length(which(Seattle_Weather$PRCP>=100))` times during the period we have data for.

Over the period we have data for we have a maximum temperature of `r max(Seattle_Weather$TMAX, na.rm=TRUE)` and a minimum of `r min(Seattle_Weather$TMIN, na.rm=TRUE)` degrees Celsius. The mean maximum temperature was `r mean(Seattle_Weather$TMAX, na.rm=TRUE)` while the mean minimum temperature was `r mean(Seattle_Weather$TMIN, na.rm=TRUE)` degrees Celsius.

``` {r SeattleDivide}
#Seattle_Weather %>% group_by(Seattle_Weather$STATION)
```

``` {r dataSummary}

#convert dates to posix dates from character class. ChatGPT used [OpenAI. ChatGPT (GPT-4). 2024. OpenAI, https://www.openai.com/chatgpt.]
Seattle_911 <- Seattle_911 %>%
   mutate(`Event.Clearance.Date` = as.POSIXct(`Event.Clearance.Date`, format = "%m/%d/%Y %I:%M:%S %p", tz = "America/Los_Angeles"))
#Stop R converting Offense Number etc into Scientific Notation
options(scipen = 999)
summary(Seattle_911)

(unique(Seattle_911$`Event.Clearance.Description`))
```

The Seattle 911 dataset contains data from `r min(Seattle_911$"Event.Clearance.Date", na.rm = TRUE)` through to `r max(Seattle_911$"Event.Clearance.Date", na.rm = TRUE)`.  During this period, `r length(Seattle_911$"CAD.Event.Number")` CAD events were recorded.


``` {r graphEventClearaceDescription}

# Aggregate the data to get counts for each 'Event Clearance Desc'
event_counts <- Seattle_911 %>%
  count(`Event.Clearance.Description`, name = "count") %>%
  arrange(desc(count))

# Convert the count column to numeric if it isn't already
event_counts$count <- as.numeric(event_counts$count)

#Group Categories to provide simplified view (120 categories, some duplicated)
event_counts <- event_counts %>%
  mutate(GroupedDescription = case_when(
    grepl("^ALACAD", `Event.Clearance.Description`, ignore.case = TRUE) ~ "ALACAD",
    grepl("^ALARMS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Alarms",
    grepl("^ANIMALS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Animals",
    grepl("^ASSAULTS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Assaults",
    grepl("^AUTO", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Auto",
    grepl("^AWOL", `Event.Clearance.Description`, ignore.case = TRUE) ~ "awol",
    grepl("^BURGLARY", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Burglary",
    grepl("^CASUALTY", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Casualty",
    grepl("^CRISIS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Crisis",
    grepl("^DISTURBANCE", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Disturbance",
    grepl("^DOMESTIC", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Domestic",
    grepl("^TRAFFIC", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Traffic",
    grepl("^HARASSMENT", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Harassment",
    grepl("^HARBOR", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Harbor",
    grepl("^LIQUOR", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Liquor",
    grepl("^MENTAL", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Mental",
    grepl("^NARCOTICS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Narcotics",
    grepl("^PARKS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Parks",
    grepl("^PERSON", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Person",
    grepl("^PROPERTY", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Property",
    grepl("^STRONG", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Strong Arm",
    grepl("^SUSPICIOUS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Suspicious",  
    grepl("^THEFT", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Theft",
    grepl("^TRESPASS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Trespass",
    TRUE ~ `Event.Clearance.Description`
  )) %>%
  # Filter out rows with NA or empty GroupedDescription
  filter(!is.na(GroupedDescription) & GroupedDescription != "") %>%
  # Group by GroupedDescription and Summarise
  group_by(GroupedDescription) %>%
  summarise(total_count = sum(count , na.rm = TRUE))  


# View the result
print(event_counts)

#event_count_graph <- ggplot(event_counts,aes(GroupedDescription)) +
 # geom_bar(stat = identity, fill = "orange") +
#  coord_flip() 
#Sevent_count_graph  

event_count_graph <- ggplot(event_counts, aes(x = GroupedDescription, y = total_count)) +
  geom_col(fill = "orange") +
  coord_flip() +
  labs(title = "Event Count by Grouped Description",
       x = "Grouped Description",
       y = "Total Count") +
  theme(axis.text.y = element_text(size = 8), # Reduce text size
        axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels

# Print the graph
print(event_count_graph)

#event_counts_summary <- event_counts %>%
#  group_by(GroupedDescription) %>%
#  summarise(TotalCount = sum(count))


# Count occurrences of each description
#top_descriptions <- event_counts %>%
#  count(`Event.Clearance.Description`, name = "Count") %>%
#  arrange(desc(Count)) %>%
#  slice_head(n = 10)

# Print the top 10 descriptions
#print(top_descriptions)

# Plotting if necessary
#ggplot(top_descriptions, aes(x = reorder(`Event.Clearance.Description`, -Count), y = Count)) +
#  geom_bar(stat = "identity") +
#  coord_flip() +  # Flip coordinates for better readability
#  theme_minimal() +
#  labs(title = "Top 10 Most Common Event Clearance Descriptions",
#       x = "Event Clearance Description",
#       y = "Count")

```

To do -
investigation of police data
map weather station locations
correlate weather station locations with police data


``` {r SplitStations}
# Group Data by weather station
weather_data_grouped <- Seattle_Weather %>%
  group_by(STATION)

# find average maximum temperature  
average_max_temp <- weather_data_grouped %>%
  summarise(avg_max_temp = mean(TMAX, na.rm = TRUE))

# Get unique station codes
station_codes <- unique(Seattle_Weather$STATION)

# Create a list to store data frames for each station
station_data_list <- list()

# Loop through each station code and filter data for that station
for (station_code in station_codes) {
  station_data <- filter(Seattle_Weather, STATION == station_code)
  station_data_list[[station_code]] <- station_data
}
ggplot(station_data_list[["USC00450872"]], aes(x=DATE, y=TMAX)) +
 geom_line() + 
 theme_bw()
 
 USC00450872 <- station_data_list[["USC00450872"]]
 
historical = xts(USC00450872[,c("TMAX","TMIN","PRCP")], order.by=as.Date(USC00450872$DATE))

historical = ts_regular(historical)

historical = suppressWarnings(na.fill(historical, "extend"))

historical = window(historical, start=as.Date("2000-01-01"), end=as.Date("2020-12-31")) 



plot(ts_ts(historical$TMAX), col="darkred", bty="n", las=1, fg=NA, 
	ylim=c(-20, 120), ylab="Temperature (F)")

lines(ts_ts(historical$TMIN), col="navy")

grid(nx=NA, ny=NULL, lty=1, col="gray")

legend("topright", fill=c("darkred", "navy"), cex=0.7,
	legend=c("TMAX", "TMIN"), bg="white")

barplot(historical$PRCP, border=NA, col="darkgreen", ylim=c(0, 2),
	space=0, bty="n", las=1, fg=NA, ylab="Daily Rainfall (inches)")

grid(nx=NA, ny=NULL, lty=1)

```

## References

1. 
2. 
3.
4. https://www.neonscience.org/resources/learning-hub/tutorials/da-viz-coop-precip-data-r