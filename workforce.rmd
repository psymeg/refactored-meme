---
title: "Chose Your Own Project - Machine Learning Submission"
subtitle: "HarvardX Data Science Capstone - PH125.9x"
author: "Simon Gibson"
date: "`r Sys.Date()`"
output: pdf_document
number_sections: true
toc: true
---
  
## Introduction

For the 9th Course in the HarvardX Data Science course we have been asked to create two recommendation systems. The first was a Movie Recommendation System using the MovieLens dataset. The second is a "Choose your Own Project." For this a we have chosen a Workforce Recommendation System - mixing weather forecasts with Police 911 call information to see if it is possible to predict Police staffing requirements based on weather based trends.

We are using the Seattle Police Department 911 Incident Response data set found here : https://www.kaggle.com/datasets/sohier/seattle-police-department-911-incident-response

For Weather data we will use National Oceanic and Atmospheric Administration (NOAA) data. Michael Minns' tutorial is inciteful for weather analysis. It can be found here: https://michaelminn.net/tutorials/r-weather/index.html This weather data does not appear to be available via an api call or similar and is quite a manual download process. Due to download constraints we will be using a locally sourced dataset covering the years 2001 to 2002.

In order to test the results of the recommendation system we are using the root-mean-square error (RMSE) to measure the difference between the values predicted by the model and the observed values.

## Method

The first step is to clear any set variables so we do not introduce anything unexpected into the data we are working with.

```{r clear, echo=FALSE}
#clear R variables
#rm(list = ls ())
```

Then we install the packages required to manipulate the data.

```{r packageInstall, echo=FALSE, results="hide", message=FALSE}
####################################################
# This code is divided into the following sections #
# 1. Install required packages                     #
# 2. edx code for creating data sets               #
# 3. Data set exploration                          #
####################################################

##########################################################
# 1. Install required packages and download data            #
##########################################################

# Note: this process takes a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "https://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "https://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "https://cran.us.r-project.org") 
if(!require(kableExtra)) install.packages("kableExtra", repos = "https://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "https://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "https://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(xts)) install.packages("xts", repos = "http://cran.us.r-project.org")
if(!require(tsbox)) install.packages("tsbox", repos = "http://cran.us.r-project.org")
if(!require(forecast)) install.packages("forecast", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(measurements)) install.packages("measurements", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(ggmap)) install.packages("ggmap", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(dplyr)
library(kableExtra)
library(lubridate)
library(scales)
library(stringr)
library(readr)
library(xts)
library(tsbox)
library(forecast)
library(data.table)
library(measurements)
library(kableExtra)
library(ggmap)

```

Following that, the data is downloaded and then divided into 2 sets. The first set is used to train the algorithm and the second set is used to validate the algorithm. By dividing the data the problem of over-training and thus producing skewed results can be avoided.

The creation of the 2 sets involves the following steps. Initially required packages are installed if not installed and then loaded. Next the data is downloaded if the zip files are not found. Column names are set and the data is converted into forms more easily processed. Then the data is joined. Finally the joined data is split into 2 sets - the edx set used to train the algorithm and the final_holdout_test set that will be used to validate the algorithm and calculate the final RMSE score.

```{r dataDownload, echo=FALSE, results="hide", message=FALSE, warning=FALSE}

#Seattle Police Department 911 Incident Response
#https://www.kaggle.com/datasets/sohier/seattle-police-department-911-incident-response/download?datasetVersionNumber=1

#National Oceanic and Atmospheric Administration (NOAA) data
#https://www.ncei.noaa.gov/orders/cdo/3533326.csv

options(timeout = 120)

dl <- "archive.zip"
if(!file.exists(dl))
  download.file("https://www.kaggle.com/datasets/sohier/seattle-police-department-911-incident-response/download?datasetVersionNumber=1", dl)

dl <- "3533326.csv"
if(!file.exists(dl))
  download.file("https://www.ncei.noaa.gov/orders/cdo/3533326.csv", dl)

```

``` {r loadData, echo=FALSE, results="hide", message=FALSE, warning=TRUE}

#Load Seattle 0911 Call data; Remove spaces from Column Labels
Seattle_911 <- read.csv("Seattle_Police_Department_911_Incident_Response.csv", check.names=TRUE)

#Load weather data
Weather <- read.csv("3533326.csv", as.is=T)

```



``` {r dataInvWeather, echo=FALSE, results="hide", message=FALSE, warning=TRUE}
#head(Weather)

#names(Weather)

#min(range(Weather$DATE))

#max(range(Weather$DATE))
```
\newpage

## Data Investigation

### Weather Dataset

Looking at the first 5 rows of data we can see the following: `r knitr::kable(head(Weather), format="latex", booktabs=TRUE) %>% 
  kable_styling(latex_options="scale_down") %>%
  kable_styling(latex_options="HOLD_position")`

The weather data starts from `r min(range(Weather$DATE))` and ends `r max(range(Weather$DATE))`. It has the following column headers: `r colnames(Weather)`. If we look at a single row, row 100 in this case, we can see the following: `r Weather[100,]`

For our investigation the following columns may be of interest - STATION, NAME, DATE, PRCP, SNOW, TMAX  TMIN. The PRCP (Precipitation) and SNOW (Snowfall) columns are in inches and the TMAX (Maximum Temperature) TMIN (Minimum Temperature) are in Fahrenheit. These will be converted to their metric equivalents using the measurements package. We are interested in determining if there is any relationship between crime reports and weather so we will determine the closest weather station using latitude and longitude bearings returned by the ggmap package.

``` {r Seattle_Weather, echo=FALSE, results="hide", message=FALSE, warning=TRUE}
#Extract the Weather Data we are interested in
Seattle_Weather <- xts(Weather[,c("NAME","STATION","DATE","TMAX","TMIN","PRCP", "SNOW")], order.by=as.Date(Weather$DATE))
#Convert to DataFrame
Seattle_Weather <- as.data.frame(Seattle_Weather)
#Convert Date to DATE class and PRCP and SNOW
Seattle_Weather$DATE <- as.Date(Seattle_Weather$DATE)
Seattle_Weather$PRCP <- as.numeric(Seattle_Weather$PRCP)
Seattle_Weather$SNOW <- as.numeric(Seattle_Weather$SNOW)

#Convert Precipitation and Snowfall from Imperial to Metric
Seattle_Weather$PRCP <- conv_unit(Seattle_Weather$PRCP, "inch", "mm")
Seattle_Weather$SNOW <- conv_unit(Seattle_Weather$SNOW, "inch", "mm")

Seattle_Weather$TMAX <- as.numeric(Seattle_Weather$TMAX)
Seattle_Weather$TMAX <- conv_unit(Seattle_Weather$TMAX, "F", "C")

Seattle_Weather$TMIN <- as.numeric(Seattle_Weather$TMIN)
Seattle_Weather$TMIN <- conv_unit(Seattle_Weather$TMIN, "F", "C")

#Extract Unique Station Names and Identifiers
Seattle_Stations <- unique(Seattle_Weather[, c('NAME', 'STATION')])

# Remove the index column - otherwise it gets printed even though we asked for only Station and Name
#rownames(Seattle_Stations) <- NULL

#ggplot(Seattle_Weather, aes(x=DATE,y=PRCP)) +
#  geom_line() +
#  xlab("Date") +
#  ylab("Daily Rainfall (Millimeters)") 

#options(digits=2)

# Add Latitude and Longitude coordinates for Weather Stations. It would be more accurate to do this manually, and if for example we were going actually use this report we would do that, however for the benefit of learning and study using the Google Maps API was preferred.
# https://console.cloud.google.com/google/maps-apis/credentials key. Set in .rProfile 
register_google(key = APIKEY)

dl <- "geocoded_stations.csv"
if(file.exists(dl)){
  geocoded_data <- read.csv("geocoded_stations.csv", as.is=T)
} else{
  options(digits=10)
  #call google maps api to get lat and longitude of stations; returned lat and long is accurate to 2 decimal places - within 1.1 kms.
  geocoded_data <- Seattle_Stations %>%
    mutate_geocode(NAME, output = "latlona", source = "google")
  #write locations to file - latitude and longitude shouldn't change. 
  write_csv(geocoded_data, file = "geocoded_stations.csv")
}

#Merge the Latitude and Longitude entries back into the Seattle_Weather dataframe.
# Seattle_Weather <- Seattle_Weather %>% didnt work so lets create temporary dataframe
Seattle_Weather_with_coords <- Seattle_Weather %>%
  left_join(geocoded_data %>% select(STATION, lat, lon), by = "STATION")
#rename it back
Seattle_Weather <- Seattle_Weather_with_coords

options(digits=2)

```

We have data from `r n_distinct(Weather$STATION)` stations: 

`r kable(Seattle_Stations, format = "markdown", booktabs=TRUE)` 

Of `r length(Seattle_Weather$PRCP)` rainfall measurements, `r length(which(Seattle_Weather$PRCP!=0))` recorded rainfall, and `r length(which(Seattle_Weather$PRCP==0))` recorded no rainfall. The maximum rainfall during this period was `r max(Seattle_Weather$PRCP, na.rm =T)`mm which fell on `r Seattle_Weather[which.max(Seattle_Weather$PRCP), "DATE"]` at `r Seattle_Weather[which.max(Seattle_Weather$PRCP), "NAME"]`. Heavy rainfall is defined by NIWA as rainfall of over 100mm in 24 hours^[https://niwa.co.nz/natural-hazards/extreme-weather-heavy-rainfall] and this occurred  `r length(which(Seattle_Weather$PRCP>=100))` times during the period we have data for.

Of `r length(Seattle_Weather$SNOW)` snowfall entries, `r length(which(Seattle_Weather$SNOW!=0))` recorded snowfall, and `r length(which(Seattle_Weather$SNOW==0))` recorded no snowfall, with `r sum(is.na(Seattle_Weather$SNOW))` not recording data. The maximum snowfall during this period was `r max(Seattle_Weather$SNOW, na.rm =T)`mm which fell on `r Seattle_Weather[which.max(Seattle_Weather$SNOW), "DATE"]` at `r Seattle_Weather[which.max(Seattle_Weather$SNOW), "NAME"]`.

Over the period we have data for we have a maximum temperature of `r max(Seattle_Weather$TMAX, na.rm=TRUE)` at `r Seattle_Weather[which.max(Seattle_Weather$TMAX), "NAME"]` and a minimum of `r min(Seattle_Weather$TMIN, na.rm=TRUE)` degrees Celsius at `r Seattle_Weather[which.max(Seattle_Weather$TMAX), "NAME"]`. The mean maximum temperature was `r mean(Seattle_Weather$TMAX, na.rm=TRUE)` while the mean minimum temperature was `r mean(Seattle_Weather$TMIN, na.rm=TRUE)` degrees Celsius.

``` {r SeattleDivide}

#We have data from `r n_distinct(Weather$STATION)` stations: 

#r kable(Seattle_Stations, format = "markdown")` 
#Seattle_Weather %>% group_by(Seattle_Weather$STATION)
```

``` {r dataSummary}

#convert dates to posix dates from character class. ChatGPT used [OpenAI. ChatGPT (GPT-4). 2024. OpenAI, https://www.openai.com/chatgpt.]
Seattle_911 <- Seattle_911 %>%
   mutate(`Event.Clearance.Date` = as.POSIXct(`Event.Clearance.Date`, format = "%m/%d/%Y %I:%M:%S %p", tz = "America/Los_Angeles"))
#Stop R converting Offense Number etc into Scientific Notation
options(scipen = 999)
summary(Seattle_911)

(unique(Seattle_911$`Event.Clearance.Description`))
```

The Seattle 911 dataset contains data from `r min(Seattle_911$"Event.Clearance.Date", na.rm = TRUE)` through to `r max(Seattle_911$"Event.Clearance.Date", na.rm = TRUE)`.  During this period, `r length(Seattle_911$"CAD.Event.Number")` CAD events were recorded.


``` {r graphEventClearaceDescription}

# Aggregate the data to get counts for each 'Event Clearance Desc'
event_counts <- Seattle_911 %>%
  count(`Event.Clearance.Description`, name = "count") %>%
  arrange(desc(count))

# Convert the count column to numeric if it isn't already
event_counts$count <- as.numeric(event_counts$count)

#Group Categories to provide simplified view (120 categories, some duplicated)
event_counts <- event_counts %>%
  mutate(GroupedDescription = case_when(
    grepl("^ALACAD", `Event.Clearance.Description`, ignore.case = TRUE) ~ "ALACAD",
    grepl("^ALARMS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Alarms",
    grepl("^ANIMALS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Animals",
    grepl("^ASSAULTS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Assaults",
    grepl("^AUTO", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Auto",
    grepl("^AWOL", `Event.Clearance.Description`, ignore.case = TRUE) ~ "awol",
    grepl("^BURGLARY", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Burglary",
    grepl("^CASUALTY", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Casualty",
    grepl("^CRISIS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Crisis",
    grepl("^DISTURBANCE", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Disturbance",
    grepl("^DOMESTIC", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Domestic",
    grepl("^TRAFFIC", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Traffic",
    grepl("^HARASSMENT", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Harassment",
    grepl("^HARBOR", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Harbor",
    grepl("^LIQUOR", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Liquor",
    grepl("^MENTAL", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Mental",
    grepl("^NARCOTICS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Narcotics",
    grepl("^PARKS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Parks",
    grepl("^PERSON", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Person",
    grepl("^PROPERTY", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Property",
    grepl("^STRONG", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Strong Arm",
    grepl("^SUSPICIOUS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Suspicious",  
    grepl("^THEFT", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Theft",
    grepl("^TRESPASS", `Event.Clearance.Description`, ignore.case = TRUE) ~ "Trespass",
    TRUE ~ `Event.Clearance.Description`
  )) %>%
  # Filter out rows with NA or empty GroupedDescription
  filter(!is.na(GroupedDescription) & GroupedDescription != "") %>%
  # Group by GroupedDescription and Summarise
  group_by(GroupedDescription) %>%
  summarise(total_count = sum(count , na.rm = TRUE))  


# View the result
print(event_counts)

#event_count_graph <- ggplot(event_counts,aes(GroupedDescription)) +
 # geom_bar(stat = identity, fill = "orange") +
#  coord_flip() 
#Sevent_count_graph  

event_count_graph <- ggplot(event_counts, aes(x = GroupedDescription, y = total_count)) +
  geom_col(fill = "orange") +
  coord_flip() +
  labs(title = "Event Count by Grouped Description",
       x = "Grouped Description",
       y = "Total Count") +
  theme(axis.text.y = element_text(size = 8), # Reduce text size
        axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels

# Print the graph
print(event_count_graph)

#event_counts_summary <- event_counts %>%
#  group_by(GroupedDescription) %>%
#  summarise(TotalCount = sum(count))


# Count occurrences of each description
#top_descriptions <- event_counts %>%
#  count(`Event.Clearance.Description`, name = "Count") %>%
#  arrange(desc(Count)) %>%
#  slice_head(n = 10)

# Print the top 10 descriptions
#print(top_descriptions)

# Plotting if necessary
#ggplot(top_descriptions, aes(x = reorder(`Event.Clearance.Description`, -Count), y = Count)) +
#  geom_bar(stat = "identity") +
#  coord_flip() +  # Flip coordinates for better readability
#  theme_minimal() +
#  labs(title = "Top 10 Most Common Event Clearance Descriptions",
#       x = "Event Clearance Description",
#       y = "Count")

```

To do -
investigation of police data
map weather station locations
correlate weather station locations with police data


``` {r SplitStations}
# Group Data by weather station
weather_data_grouped <- Seattle_Weather %>%
  group_by(STATION)

# find average maximum temperature  
average_max_temp <- weather_data_grouped %>%
  summarise(avg_max_temp = mean(TMAX, na.rm = TRUE))

# Get unique station codes
station_codes <- unique(Seattle_Weather$STATION)

# Create a list to store data frames for each station
station_data_list <- list()

# Loop through each station code and filter data for that station
for (station_code in station_codes) {
  station_data <- filter(Seattle_Weather, STATION == station_code)
  station_data_list[[station_code]] <- station_data
}
ggplot(station_data_list[["USC00450872"]], aes(x=DATE, y=TMAX)) +
 geom_line() + 
 theme_bw()
 
 USC00450872 <- station_data_list[["USC00450872"]]
 
historical = xts(USC00450872[,c("TMAX","TMIN","PRCP")], order.by=as.Date(USC00450872$DATE))

historical = ts_regular(historical)

historical = suppressWarnings(na.fill(historical, "extend"))

historical = window(historical, start=as.Date("2000-01-01"), end=as.Date("2020-12-31")) 



plot(ts_ts(historical$TMAX), col="darkred", bty="n", las=1, fg=NA, 
	ylim=c(-20, 120), ylab="Temperature (F)")

lines(ts_ts(historical$TMIN), col="navy")

grid(nx=NA, ny=NULL, lty=1, col="gray")

legend("topright", fill=c("darkred", "navy"), cex=0.7,
	legend=c("TMAX", "TMIN"), bg="white")

barplot(historical$PRCP, border=NA, col="darkgreen", ylim=c(0, 2),
	space=0, bty="n", las=1, fg=NA, ylab="Daily Rainfall (inches)")

grid(nx=NA, ny=NULL, lty=1)

```

## References

1. 
2. 
3.
4. https://www.neonscience.org/resources/learning-hub/tutorials/da-viz-coop-precip-data-r